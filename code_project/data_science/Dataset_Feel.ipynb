{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script de travail sur le dataset feel\n",
    "Ce script a pour but d'étudier le dataset feel, en terme de traitement NLP, de vectorisation, d'apprentissage de modèles de machine learning et leur impact sur le dataset de test.\n",
    "\n",
    "#### Importation des librairies utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# Pour le traitement de données\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Pour choisir les modèles de ML\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Pour gérer les fichiers et évaluer la performance temporelles\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), \"..\"))\n",
    "\n",
    "# Scripts permettant de traiter les données\n",
    "from code_project.data_science.utils_project import load_saved_file, read_feel, process_label, process_nlp, vectorize, apply_vectorization\n",
    "from code_project.data_science.evaluate import load_test_set, evaluate_model\n",
    "from code_project.config_project import *\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overwrite du fichier de configuration\n",
    "On écrit sur des données nouvelles pour éviter de casser les modèles pré-entrainés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_VECTOR = \"tf-idf\"\n",
    "VECTOR_NAME = \"test_2.sav\"\n",
    "MODEL_NAME = \"model_2.sav\"\n",
    "\n",
    "SAVE_DIR = os.path.join(os.path.dirname(CWD_PATH), \"data\", \"modeles\")\n",
    "MODEL_PATH = os.path.join(SAVE_DIR, MODEL_NAME)\n",
    "VECTOR_PATH = os.path.join(SAVE_DIR, VECTOR_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Options permettant de gérer le traitement des données ou la prédiction\n",
    "- PREDICT_WORD : permet de prédire sur chaque mot (plutôt que sur chaque phrase). On agrège ensuite les prédictions à la phrase en récupérant le sentiment le plus présent.\n",
    "- UP_SAMPLING : les classes des données étant déséquilibrées, on les ré-équilibre en augmentant le nombre de labels des classes moins présentes.\n",
    "- ONE_VS_ALL : on prédit sur chaque classe et on prend la classe ayant la probabilité la plus grande plutôt que chercher à prédire la classe directement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICT_WORD = True\n",
    "UP_SAMPLING = True\n",
    "ONE_VS_ALL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lecture des données\n",
    "Lecture du dataset FEEL et des données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files read, duration : 0.0766s\n"
     ]
    }
   ],
   "source": [
    "# Lecture des données\n",
    "t1 = time.time()\n",
    "feel = read_feel(FEEL_PATH, limit=LIMIT)\n",
    "data_test = load_test_set(TEST_SET_PATH)\n",
    "t2 = time.time()\n",
    "print(\"Files read, duration : {}s\".format(round(t2-t1, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traitement des données\n",
    "- On traite en premier les labels pour dédoubler les mots pouvant avoir plusieurs émotions\n",
    "- On effectue un traitement NLP sur les phrases (tokenisation, normalisation, stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data processed, duration : 62.0088s\n"
     ]
    }
   ],
   "source": [
    "t3 = time.time()\n",
    "feel = process_label(feel)\n",
    "feel['nlp_sentence'] = process_nlp(feel['sentence'])\n",
    "data_test['nlp_sentence'] = process_nlp(data_test['phrase'])\n",
    "t4 = time.time()\n",
    "print(\"Data processed, duration : {}s\".format(round(t4-t3, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Séparation des données en features + label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = feel.nlp_sentence, feel.emotion\n",
    "x_test, y_test = data_test.nlp_sentence, data_test.emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorisation des données\n",
    "Suivant la méthode de vectorisation choisie on vectorise les mots : \n",
    "- CountVectorizer\n",
    "- TfIdf\n",
    "- Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text vectorized, duration : 0.1883s\n"
     ]
    }
   ],
   "source": [
    "t5 = time.time()\n",
    "feat_train, vectorizer = vectorize(x_train, type_vector=TYPE_VECTOR)\n",
    "t6 = time.time()\n",
    "print(\"Text vectorized, duration : {}s\".format(round(t6-t5, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample des données\n",
    "Les classes sont déséquilibrées ici, on rajoute des labels dans les classes les moins présentes pour éviter l'overtfitting. La méthode choisie après plusieurs essais est celle de rajouter aléatoirement des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sadness       7912\n",
      "disgust       7912\n",
      "joy           7912\n",
      "surprise      7912\n",
      "anger         7912\n",
      "no_emotion    7912\n",
      "fear          7912\n",
      "Name: emotion, dtype: int64\n",
      "Up-sampling done, duration : 0.0404207706451416\n"
     ]
    }
   ],
   "source": [
    "if UP_SAMPLING:\n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    t7 = time.time()\n",
    "    upsamp = RandomOverSampler(random_state=777)\n",
    "    \n",
    "    feat_train, y_train = upsamp.fit_sample(feat_train, y_train)\n",
    "    t8 = time.time()\n",
    "    \n",
    "    print(pd.DataFrame({'emotion' : y_train}).emotion.value_counts())\n",
    "    print(\"Up-sampling done, duration : {}\".format(t8-t7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modèles de machine learning\n",
    "Après plusieurs essais, les 3 modèles les plus probants sont le RandomForest, le SVM et la régression logistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rand_for = RandomForestClassifier(n_jobs=-1, n_estimators=100)\n",
    "svc = SVC()\n",
    "log_reg = LogisticRegression(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Définition de la fonction de prédiction\n",
    "Adaptation de la fonction de prédiction de chaque phrases dans le cas où l'option PREDICT_WORD est validée : on prédit sur chaque mot et on agrège les prédictions en conservant l'émotion la plus présente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(lst):\n",
    "    lst = [el for el in lst if el != 'no_emotion']\n",
    "    if len(lst) == 0:\n",
    "        return 'no_emotion'\n",
    "    else:\n",
    "        return max(set(lst), key=lst.count)\n",
    "    \n",
    "def predict(x_test, vectorizer, model, predict_word):\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for sentence in x_test:\n",
    "        if predict_word:\n",
    "            list_emotion_word = []\n",
    "            for word in sentence:\n",
    "                feat_word = vectorizer.transform([word])\n",
    "                emotion_word = model.predict(feat_word)\n",
    "                list_emotion_word.append(list(emotion_word)[0])\n",
    "            predictions.append(most_common(list_emotion_word))\n",
    "        else:\n",
    "            feat_sent = vectorizer.transform(sentence)\n",
    "            emotion_sent = model.predict(feat_sent)\n",
    "            predictions.append(list(emotion_sent)[0])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sélection du meilleur modèle\n",
    "A l'image du grid_search, cette fonction permet d'entrainer plusieurs modèles et de ne garder uniquement celui qui possède le score le plus grand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_grid(model, vectorizer, feat_train, y_train, x_test, y_test, param_grid, predict_word):\n",
    "    \n",
    "    grid = ParameterGrid(param_grid)\n",
    "    print(\"{} models to train\\n\".format(len(grid)))\n",
    "    \n",
    "    best_score, best_params = 0, None\n",
    "    \n",
    "    for params in grid:\n",
    "        print('model to train : \\n-params {}'.format(params))\n",
    "        # On change les paramètres et on fit le modèle à chaque itération\n",
    "        model.set_params(**params)\n",
    "        model.fit(feat_train, y_train)\n",
    "        \n",
    "        # On prédit sur les données de test\n",
    "        predictions = predict(x_test, vectorizer, model, predict_word)\n",
    "        score = accuracy_score(y_test, predictions)\n",
    "        mat_conf = confusion_matrix(y_test, predictions)\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params\n",
    "        \n",
    "        print('-score {}'.format(score))\n",
    "        print('confusion matrix : \\n{}'.format(mat_conf))\n",
    "    \n",
    "    print(\"\\nBEST_SCORE : {}\".format(best_score))\n",
    "    print(\"BEST_PARAMS : {}\".format(best_params))\n",
    "        \n",
    "    return best_score, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apprentissage des différents modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On conserve les paramètres optimisés ici pour une nouvelle passe plus fine de la fonction\n",
    "params_rand_for = {'bootstrap': False, 'max_features': 'auto', \n",
    "                   'min_samples_leaf': 1, 'min_samples_split': 2, \n",
    "                   'n_estimators': 100}\n",
    "params_svc = {'C': 5, 'gamma': 0.3, 'kernel': 'linear'}\n",
    "params_log_reg = {'C': 5, 'multi_class': 'auto', 'penalty': 'l1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 models to train\n",
      "\n",
      "model to train : \n",
      "-params {'bootstrap': True, 'max_features': 'auto', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2a3d57df5646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m score_rand_for, params_rand_for = solve_grid(rand_for, vectorizer, feat_train, y_train, \n\u001b[0;32m---> 11\u001b[0;31m                                              x_test, y_test, param_grid, predict_word=PREDICT_WORD)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-b5f45f35cc04>\u001b[0m in \u001b[0;36msolve_grid\u001b[0;34m(model, vectorizer, feat_train, y_train, x_test, y_test, param_grid, predict_word)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# On change les paramètres et on fit le modèle à chaque itération\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# On prédit sur les données de test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 335\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 899\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    900\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Amélioration du random forest\n",
    "param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'n_estimators': [100, 200]\n",
    "}\n",
    "\n",
    "score_rand_for, params_rand_for = solve_grid(rand_for, vectorizer, feat_train, y_train, \n",
    "                                             x_test, y_test, param_grid, predict_word=PREDICT_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 models to train\n",
      "\n",
      "model to train : \n",
      "-params {'C': 4, 'gamma': 0.3, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Amélioration du SVM\n",
    "\n",
    "param_grid = {\n",
    "    'C': [4, 6],\n",
    "    'gamma': [0.3, 0.4],\n",
    "    'kernel': ['linear', 'sigmoid']\n",
    "}\n",
    "\n",
    "score_svc, params_svc = solve_grid(svc, vectorizer, feat_train, y_train, \n",
    "                                   x_test, y_test, param_grid, predict_word=PREDICT_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 models to train\n",
      "\n",
      "model to train : \n",
      "-params {'C': 2, 'multi_class': 'auto', 'penalty': 'l1'}\n",
      "['sadness', 'sadness', 'joy']\n",
      "-score 0.2875816993464052\n",
      "confusion matrix : \n",
      "[[ 7  2  2  2  3  3  5]\n",
      " [ 9  9  1  2  2  4  2]\n",
      " [ 5  1  5  5  1  1  5]\n",
      " [ 5  1  5  5  4  3 14]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 1  2  1  2  0 11  2]\n",
      " [ 5  2  1  4  0  2  7]]\n",
      "model to train : \n",
      "-params {'C': 2, 'multi_class': 'auto', 'penalty': 'l2'}\n",
      "['sadness', 'sadness', 'joy']\n",
      "-score 0.28104575163398693\n",
      "confusion matrix : \n",
      "[[ 7  3  1  2  3  4  4]\n",
      " [10 10  1  2  2  2  2]\n",
      " [ 4  1  5  5  1  2  5]\n",
      " [ 6  2  4  4  4  3 14]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 1  4  1  1  0 10  2]\n",
      " [ 4  2  2  4  0  2  7]]\n",
      "model to train : \n",
      "-params {'C': 4, 'multi_class': 'auto', 'penalty': 'l1'}\n",
      "['sadness', 'joy', 'joy']\n",
      "-score 0.2875816993464052\n",
      "confusion matrix : \n",
      "[[ 8  2  1  3  3  3  4]\n",
      " [10  9  0  2  2  4  2]\n",
      " [ 5  1  5  7  1  1  3]\n",
      " [ 6  0  4  8  4  3 12]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 1  2  1  4  0  9  2]\n",
      " [ 5  2  1  6  0  2  5]]\n",
      "model to train : \n",
      "-params {'C': 4, 'multi_class': 'auto', 'penalty': 'l2'}\n",
      "['sadness', 'sadness', 'joy']\n",
      "-score 0.27450980392156865\n",
      "confusion matrix : \n",
      "[[ 7  3  1  2  3  4  4]\n",
      " [10 10  1  2  2  2  2]\n",
      " [ 5  1  4  5  1  2  5]\n",
      " [ 6  2  4  4  4  3 14]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 1  4  1  1  0 10  2]\n",
      " [ 4  2  2  4  0  2  7]]\n",
      "model to train : \n",
      "-params {'C': 5, 'multi_class': 'auto', 'penalty': 'l1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disgust', 'disgust', 'joy']\n",
      "-score 0.2549019607843137\n",
      "confusion matrix : \n",
      "[[ 0 19  1  2  0  1  1]\n",
      " [ 2 25  0  0  0  2  0]\n",
      " [ 2 15  1  3  0  0  2]\n",
      " [ 1 27  2  3  1  1  2]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0 10  0  2  0  6  1]\n",
      " [ 3 11  0  2  0  1  4]]\n",
      "model to train : \n",
      "-params {'C': 5, 'multi_class': 'auto', 'penalty': 'l2'}\n",
      "['sadness', 'sadness', 'joy']\n",
      "-score 0.27450980392156865\n",
      "confusion matrix : \n",
      "[[ 7  3  1  2  3  4  4]\n",
      " [10 10  1  2  2  2  2]\n",
      " [ 5  1  4  5  1  2  5]\n",
      " [ 6  2  4  4  4  3 14]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 1  4  1  1  0 10  2]\n",
      " [ 4  2  2  4  0  2  7]]\n",
      "model to train : \n",
      "-params {'C': 8, 'multi_class': 'auto', 'penalty': 'l1'}\n",
      "['disgust', 'disgust', 'joy']\n",
      "-score 0.24183006535947713\n",
      "confusion matrix : \n",
      "[[ 0 18  1  2  0  2  1]\n",
      " [ 2 24  0  1  0  2  0]\n",
      " [ 2 14  1  3  0  0  3]\n",
      " [ 1 26  2  2  1  1  4]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0 11  0  2  0  5  1]\n",
      " [ 3  9  0  2  0  2  5]]\n",
      "model to train : \n",
      "-params {'C': 8, 'multi_class': 'auto', 'penalty': 'l2'}\n",
      "['sadness', 'sadness', 'joy']\n",
      "-score 0.28104575163398693\n",
      "confusion matrix : \n",
      "[[ 8  2  1  2  3  4  4]\n",
      " [10 10  1  2  2  2  2]\n",
      " [ 5  1  4  6  1  2  4]\n",
      " [ 6  2  4  6  4  3 12]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 1  4  1  2  0  9  2]\n",
      " [ 4  2  2  5  0  2  6]]\n",
      "model to train : \n",
      "-params {'C': 10, 'multi_class': 'auto', 'penalty': 'l1'}\n",
      "['disgust', 'disgust', 'joy']\n",
      "-score 0.24183006535947713\n",
      "confusion matrix : \n",
      "[[ 0 18  1  2  0  2  1]\n",
      " [ 2 24  0  1  0  2  0]\n",
      " [ 2 14  1  3  0  0  3]\n",
      " [ 1 26  2  2  1  1  4]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0 11  0  2  0  5  1]\n",
      " [ 3 10  0  2  0  1  5]]\n",
      "model to train : \n",
      "-params {'C': 10, 'multi_class': 'auto', 'penalty': 'l2'}\n",
      "['disgust', 'disgust', 'joy']\n",
      "-score 0.2549019607843137\n",
      "confusion matrix : \n",
      "[[ 0 18  1  2  0  2  1]\n",
      " [ 2 24  0  1  0  2  0]\n",
      " [ 1 14  2  4  0  0  2]\n",
      " [ 1 26  2  4  1  1  2]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0 11  0  2  0  5  1]\n",
      " [ 3  9  1  3  0  1  4]]\n",
      "model to train : \n",
      "-params {'C': 15, 'multi_class': 'auto', 'penalty': 'l1'}\n",
      "['disgust', 'disgust', 'joy']\n",
      "-score 0.24183006535947713\n",
      "confusion matrix : \n",
      "[[ 0 18  1  2  0  2  1]\n",
      " [ 1 25  0  1  0  2  0]\n",
      " [ 2 14  1  3  0  0  3]\n",
      " [ 1 26  2  2  1  1  4]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0 11  0  2  0  5  1]\n",
      " [ 3 10  0  2  0  2  4]]\n",
      "model to train : \n",
      "-params {'C': 15, 'multi_class': 'auto', 'penalty': 'l2'}\n",
      "['disgust', 'disgust', 'joy']\n",
      "-score 0.2549019607843137\n",
      "confusion matrix : \n",
      "[[ 0 18  1  2  0  2  1]\n",
      " [ 2 24  0  1  0  2  0]\n",
      " [ 2 14  1  4  0  0  2]\n",
      " [ 1 26  2  4  1  1  2]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0 10  0  2  0  6  1]\n",
      " [ 3 10  0  3  0  1  4]]\n",
      "model to train : \n",
      "-params {'C': 20, 'multi_class': 'auto', 'penalty': 'l1'}\n",
      "['disgust', 'disgust', 'joy']\n",
      "-score 0.22875816993464052\n",
      "confusion matrix : \n",
      "[[ 0 18  1  2  0  2  1]\n",
      " [ 1 25  0  1  0  2  0]\n",
      " [ 3 14  1  3  0  0  2]\n",
      " [ 2 26  2  2  1  1  3]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 1 11  0  2  0  5  0]\n",
      " [ 4 11  0  2  0  2  2]]\n",
      "model to train : \n",
      "-params {'C': 20, 'multi_class': 'auto', 'penalty': 'l2'}\n",
      "['disgust', 'disgust', 'joy']\n",
      "-score 0.2549019607843137\n",
      "confusion matrix : \n",
      "[[ 0 18  1  2  0  2  1]\n",
      " [ 2 24  0  1  0  2  0]\n",
      " [ 2 14  1  4  0  0  2]\n",
      " [ 1 26  2  4  1  1  2]\n",
      " [ 0  0  0  0  0  0  0]\n",
      " [ 0 10  0  2  0  6  1]\n",
      " [ 3 10  0  3  0  1  4]]\n",
      "\n",
      "BEST_SCORE : 0.2875816993464052\n",
      "BEST_PARAMS : {'C': 2, 'multi_class': 'auto', 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "# Amélioration de la régression logistique\n",
    "\n",
    "param_grid = {\n",
    "    \"C\": [1, 3, 5, 6], \n",
    "    \"penalty\": [\"l1\",\"l2\"],\n",
    "    \"multi_class\": [\"auto\", \"ovr\"]\n",
    "}\n",
    "\n",
    "score_log_reg, params_log_reg = solve_grid(log_reg, vectorizer, feat_train, y_train, \n",
    "                                           x_test, y_test, param_grid, predict_word=PREDICT_WORD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrainement du modèle de bagging\n",
    "On se sert des 3 modèles optimisés ci-dessus afin de réaliser un bagging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apprentissage des modèles optimisés\n",
    "rand_for_opt = RandomForestClassifier(n_jobs=-1, **params_rand_for)\n",
    "svc_opt = SVC(probability=True,**params_svc)\n",
    "log_reg_opt = LogisticRegression(n_jobs=-1, **params_log_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('rl', LogisticRegression(C=5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='auto', n_jobs=-1,\n",
       "          penalty='l1', random_state=None, solver='warn', tol=0.0001,\n",
       "          verbose=0, warm_start=False)), ('svm', SVC(C=5, ca..._jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=-1, voting='soft',\n",
       "         weights=[1, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bagging des modèles ci-dessus\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting = VotingClassifier(estimators=[('rl', log_reg_opt),('svm', svc_opt),('rf', rand_for_opt)],\n",
    "                          voting='soft', weights = [1, 1, 1], n_jobs=-1)\n",
    "voting.fit(feat_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score voting classifier : 0.30141843971631205\n",
      "matrice de confusion : \n",
      "[[ 3  0  5  8  7  5  6  8]\n",
      " [ 0  0  0  0  0  0  1  0]\n",
      " [ 4  0 19 14  2  2  3  4]\n",
      " [ 8  0  3  9  6  1  7 11]\n",
      " [ 3  0  1  0 29 11  0 14]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 2  0  3 13  5  2  9  8]\n",
      " [ 1  0  0 13  9  5  2 16]]\n"
     ]
    }
   ],
   "source": [
    "# Prédiction et score du modèle global\n",
    "predictions = predict(x_test, vectorizer, voting, PREDICT_WORD)\n",
    "score = accuracy_score(y_test, predictions)\n",
    "mat_conf = confusion_matrix(y_test, predictions)\n",
    "\n",
    "print(\"score voting classifier : {}\".format(score))\n",
    "print(\"matrice de confusion : \\n{}\".format(mat_conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
